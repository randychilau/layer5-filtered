---
title: "Value of a Service Mesh"
thumbnail: ./service-mesh.svg
category: Service mesh
tags:
 - Service mesh
featured: false
published: true
---

import { Link } from "gatsby";
import { TopicsWrapper } from "../Topics.style.js";
import Decoupling from "./epsm_0109.png";
import Communication from "./epsm_0108.png";
import Timeouts from "./epsm_0107.png";
import Metrics from "./epsm_0106.png";
import Mixer from "./epsm_0104.png";

<TopicsWrapper>
  <div className="intro">
    <p>Learn more about service mesh fundamentals in
      <Link className="blog" to="/learn/books">The Enterprise Path to Service Mesh Archictures (2nd Edition)</Link> -  free book and excellent resource which addresses how to evaluate your organization’s readiness, provides factors to consider when building new applications and converting existing applications to best take advantage of a service mesh, and offers insight on deployment architectures used to get you there.</p>
  </div>

<p>
    Service meshes provide visibility, resiliency, traffic, and security control of distributed application services. Much value is promised here, particularly to the extent that much is given without the need to change your application code (or much of it, depending).
</p>

<h3>Observability</h3>

<p>Many organizations are initially attracted to the uniform observability that service meshes provide. No complex system is ever fully healthy. Service-level telemetry illuminates where your system is behaving sickly, illuminating difficult-to-answer questions like why your requests are slow to respond. Identifying when a specific service is down is relatively easy, but identifying where it’s slow and why, is another matter.</p>

<p>From the application’s vantage point, service meshes provide black-box monitoring (observing a system from the outside) of service-to-service communication and white-box monitoring (observing a system from the inside). Some service meshes work in combination with a distributed tracing library to deliver white-box monitoring, while other service meshes enable a deeper level of visibility through protocol specific filters as a capability of their proxies. Comprise the data plane are well-positioned (transparently, in-band) to generate metrics, logs, and traces, providing uniform and thorough observability throughout the mesh as a whole</p>
  <div className="center" >
  <img src={Mixer} align="right" alt="Istio Mixer" />
  <p>Figure 1: Istio’s Mixer is capable of collecting multiple telemetric signals and sending those signals to backend monitoring, authentication, and quota systems via adapters</p>
  </div>
<p>You are probably accustomed to having individual monitoring solutions for distributed tracing, logging, security, access control, and so on. Service meshes centralize and assist in solving these observability challenges by providing the following:</p>

  <div className="right" >
  <img src={Metrics} align="right" alt="Request Metrics" />
  <p>Figure 2: Request metrics generated by Istio and visible in Meshery</p>
  </div>
<ul>
    <li>
        <strong>Logging</strong>
        <p>
            Logs are used to baseline visibility for access requests to your entire fleet of services. Figure 1 illustrates how telemetry transmitted through service mesh logs include source and destination, request protocol, endpoint (URL), associated response code, and response time and size.
        </p>
    </li>
    <li>
        <strong>Metrics</strong>
        <p>
Metrics are used to remove dependency and reliance on the development process to instrument code to emit metrics. When metrics are ubiquitous across your cluster, they unlock new insights. Consistent metrics enables automation for things like autoscaling, as an example.Telemetry emitted by service mesh metrics include global request volume, global success rate, individual service responses by version, source and time, as shown in Figure 2.        </p>
    </li>
    <li>
        <strong>Tracing</strong>
        <p>
Without tracing, slow services (versus services that simply fail) are most difficult to debug. Imagine manual enumeration of all of your service dependencies being tracked in a spreadsheet. Traces are used to visualize dependencies, request volumes, and failure rates. With automatically generated span identifiers, service meshes make integrating tracing functionality almost effortless. Individual services in the mesh still need to forward context headers, but that’s it. In contrast, many application performance management (APM) solutions require manual instrumentation to get traces out of your services. Later, you’ll see that in the sidecar proxy deployment model, sidecars are ideally positioned to trace the flow of requests across services.        </p>
    </li>
</ul>
<h3>Traffic control</h3>

<p>
    Service meshes provide granular, declarative control over network traffic to determine where a request is routed to perform canary release, for example. Resiliency features typically include circuit breaking, latency-aware load balancing, eventually consistent service discovery, timeouts, deadlines, and retries.
</p>

<p>
    <strong>Timeouts</strong> provide cancellation of service requests when a request doesn’t return to the client within a predefined time. Timeouts limit the amount of time spent on any individual request, and are enforced at a point in time after which a response is considered invalid or too long for a client (user) to wait. Deadlines are an advanced service mesh feature in that they facilitate the feature-level timeouts (a collection of requests) rather than independent service timeouts, helping to avoid retry storms. Deadlines deduct time left to handle a request at each step, propagating elapsed time with each downstream service call as the request travels through the mesh. Timeouts and deadlines, illustrated in Figure 3, can be considered as enforcers of your Service-Level Objectives (SLOs).
</p>

<p>
    When a service times-out or is unsuccessfully returned, you might choose to retry the request. Simple retries bear the risk of making things worse by retrying the same call to a service that is already under water (retry three times = 300% more service load). Retry budgets (aka maximum retries), however, provide the benefit of multiple tries but with a limit so as to not overload what is already a load-challenged service. Some service meshes take the elimination of client contention further by introducing jitter and an exponential back-off algorithm in the calculation of timing the next retry attempt.
</p>

  <div className="left" >
  <img src={Timeouts} align="right" alt="Deadlines" />
  <p>Figure 3:Deadlines, not ubiquitously supported by different service meshes, set feature-level timeouts</p>
  </div>

<p>
    Instead of retrying and adding more load to the service, you might elect to fail fast and disconnect the service, disallowing calls to it. Circuit breaking provides configurable timeouts (or failure thresholds) to ensure safe maximums and facilitate graceful failure commonly for slow-responding services. Using a service mesh as a separate layer to implement <strong>circuit breaking</strong> avoids undue overhead on applications (services) at a time when they are already oversubscribed.
</p>

<p>
    <strong>Rate limiting</strong> (throttling) is used to ensure stability of a service so that when one client causes a spike in requests, the service continues to run smoothly for other clients. Rate limits are usually measured over a period of time, but you can use different algorithms (fixed or sliding window, sliding log, etc.). Rate limits are typically operationally focused on ensuring that your services aren’t oversubscribed.
</p>

<p>When a limit is reached, well-implemented services commonly adhere to IETF RFC 6585, sending 429 Too Many Requests as the response code, including headers, such as the following, describing the request limit, number of requests remaining, and amount of time remaining until the request counter is reset:
</p>

<div class="fact-left">
<p>X-RateLimit-Limit: 60</p>
<p>X-RateLimit-Remaining: 0</p>
<p>X-RateLimit-Reset: 1372016266</p>
</div>

<p>
    Rate limiting protects your services from overuse by limiting how often a client (most often identified by a user access token) can call your service(s), and provides operational resiliency (e.g., service A can handle only 500 requests per second).
</p>

<p>A slightly different approach is quota management (or conditional rate limiting) that is primarily used for accounting of requests based on business requirements as opposed to limiting rates based on operational concerns. It can be difficult to distinguish between rate limiting and quota management, given that these two features can be implemented by the same service mesh capability but presented differently to users.</p>

<p>The canonical example of a quota management is to configure a policy setting a threshold for the number of client requests allowed to a service over the course of time, like user Lee is subscribed to the Free service plan and allowed only 10 requests per day. Quota policy enforces consumption limits on services by maintaining a distributed counter that tallies incoming requests often using an in-memory datastore like Redis. Conditional rate limits are a powerful service mesh capability when implemented based on a user-defined set of arbitrary attributes.
</p>

  <div className="intro">
      <h3>Conditional Rate Limiting Example: Implementing Class of Service</h3>
    <p>In this example, let’s consider a “temperature-check” service that provides a readout of the current temperature for a given geographic area, updated on one-minute intervals. The service provides two different experiences to clients when interacting with its API: an authenticated, but unentitled (free account) experience, and an entitled (paying account) experience like so:
    </p>
    <ul>
    <li><p>If the request on the temperature-check service is unauthenticated, the service limits responses to a given requester (client) to one request every 600 seconds. Any unauthenticated user is restricted to receiving an updated result at 10-minute intervals to spare the temperature-check service’s resources and provide paying users with a premium experience.</p></li>
    <li><p>Authenticated users (perhaps, those providing a valid authentication token in the request) are those who have active service subscriptions (paying customers) and therefore are entitled to up-to-the-minute updates on the temperate-check service’s data (authenticated requests to the temperature-check service are not rate limited).</p></li>
    </ul>
    <p>In this example, through conditional rate limiting, the service mesh is providing a separate class of service to paying and non-paying clients of the temperature-check service. There are many ways in which class of service can be provided by the service mesh (e.g., authenticated requests are sent to a separate service, “temperature-check-premium”).</p>
    <p>Generally expressed as rules within a collection of policies, traffic control behavior is defined in the control plane and pushed as configuration to the data plane. The order of operations for rule evaluation is specific to each service mesh, but it is often evaluated from top to bottom.</p>
  </div>

  <h3>Security</h3>

  <div className="right" >
  <img src={Communication} align="right" alt="Communication Paths" />
  <p>Figure 4: An example of service mesh architecture. Secure communication paths in Istio</p>
  </div>

  <p>
      Most service meshes provide a certificate authority to manage keys and certificates for securing service-to-service communication. Certificates are generated per service and provide a unique identity of that service. When sidecar proxies are used (discussed later in Chapter 3), they take on the identity of the service and perform lifecycle management of certificates (generation, distribution, refresh, and revocation) on behalf of the service. In sidecar proxy deployments, you’ll typically find that local TCP connections are established between the service and sidecar proxy, whereas mutual Transport Layer Security (mTLS) connections are established between proxies.
  </p>

  <p>Encrypting traffic internal to your application is an important security consideration. Your application’s service calls are no longer kept inside a single monolith via localhost; they are exposed over the network. Allowing service calls without TLS on the transport is setting yourself up for security problems. When two mesh-enabled services communicate, they have strong cryptographic proof of their peers. After identities are established, they are used in constructing access control policies, determining whether a request should be serviced. Depending on the service mesh used, policy controls configuration of the key management system (e.g., certificate refresh interval) and operational access control are used to determine whether a request is accepted. White and blacklists are used to identify approved and unapproved connection requests as well as more granular access control factors like time of day.
  </p>

  <h3>Delay and fault injection</h3>

  <p>
     The notion that your networks and/or systems will fail must be embraced. Why not preemptively inject failure and verify behavior? Given that proxies sit in line to service traffic, they often support protocol-specific fault injection, allowing configuration of the percentage of requests that should be subjected to faults or network delay. For instance, generating HTTP 500 errors helps to verify the robustness of your distributed application in terms of how it behaves in response.
  </p>

  <p>
     Injecting latency into requests without a service mesh can be a tedious task but is probably a more common issue faced during operation of an application. Slow responses that result in an HTTP 503 after a minute of waiting leaves users much more frustrated than a 503 after a few seconds. Arguably, the best part of these resilience testing capabilities is that no application code needs to change in order to facilitate these tests. Results of the tests, on the other hand, might well have you changing application code.
  </p>

  <p>
     Using a service mesh, developers invest much less in writing code to deal with infrastructure concerns—code that might be on a path to being commoditized by service meshes. The separation of service and session-layer concerns from application code manifests in the form of a phenomenon I refer to as a decoupling at Layer 5.
  </p>

  <h2>Decoupling at Layer 5</h2>
  <p>Service meshes help you to avoid bloated service code, fat on infrastructure concerns.</p>
  <p>Duplicative work is avoided in making services production-ready by way of singularly addressing load balancing, autoscaling, rate limiting, traffic routing, and so on. Teams avoid inconsistency of implementation across different services to the extent that the same set of central control is provided for retries and budgets, failover, deadlines, cancellation, and so forth. Implementations done in silos lead to fragmented, non-uniform policy application and difficult debugging.</p>

  <p>Service meshes insert a dedicated infrastructure layer between dev and ops, separating what are common concerns of service communication by providing independent control over them. The service mesh is a networking model that sits at a layer of abstraction above TCP/IP. Without a service mesh, operators are still tied to developers for many concerns as they need new application builds to control network traffic, shaping, affecting access control, and which services talk to downstream services. The decoupling of dev and ops is key to providing autonomous independent iteration.</p>

  <p>Decoupling is an important trend in the industry. If you have a significant number of services, you have at least these three roles: developers, operators, and service owners (product owners). Just as microservices is a trend in the industry for allowing teams to independently iterate, so do service meshes allow teams to decouple and iterate faster. Technical reasons for having to coordinate between teams dissolves in many circumstances, like the following short list of examples:</p>
  <ul>
      <li>Operators don’t necessarily need to involve Developers to change how many times a service should retry before timing out or to run experiments (known as chaos engineering). They are empowered to affect service behavior independently.
</li>
      <li>Customer Success (support) teams can handle the revocation of client access without involving Operators.</li>
      <li>Product Owners can use quota management to enforce price plan limitations for quantity-based consumption of particular services.</li>
      <li>Developers can redirect their internal stakeholders to a canary with beta functionality without involving Operators.</li>
      <li>Security Engineers can declaratively define authentication and authorization policies, enforced by the service mesh.</li>
      <li>Network Engineers are empowered with an extraordinarily high degree of application-level control formerly simply unavailable to them.
</li>
  </ul>

  <p>Microservices decouple functional responsibilities within an application from one another, allowing development teams to independently iterate and move forward. Figure 1-10 shows that in the same fashion, service meshes decouple functional responsibilities of instrumentation and operating services from developers and operators, providing an independent point of control and centralization of responsibility. </p>
  
  <div className="left" >
  <img src={Decoupling} align="right" alt="Decoupling" />
  <p>Figure 5: Decoupling as a way of increasing velocity</p>
  </div>
  <p>Even though service meshes facilitate a separation of concerns, both developers and operators should understand the details of the mesh. The more everyone understands, the better. Operators can obtain uniform metrics and traces from running applications involving diverse language frameworks without relying on developers to manually instrument their applications. Developers tend to consider the network as a dumb transport layer that really doesn’t help with service-level concerns. We need a network that operates at the same level as the services we build and deploy. Because service meshes are capable of deep packet inspection and mutation at the application level, service owners are empowered to bypass developers and affect business-level logic behavior without code change. </p>

  <p>Essentially, you can think of a service mesh as surfacing the session layer of the OSI model as a separately addressable, first-class citizen in your modern architecture. As a highly configurable work horse, they are a secret weapon of cloud native architectures, waiting to be exploited.</p>
</TopicsWrapper>
